{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d3aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import os\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dba39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b8beb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rawdata/original_raw_files/full_data/CMS.csv',\n",
       " 'rawdata/original_raw_files/full_data/Condition.csv',\n",
       " 'rawdata/original_raw_files/full_data/Credit.csv',\n",
       " 'rawdata/original_raw_files/full_data/Demo.csv',\n",
       " 'rawdata/original_raw_files/full_data/Lab.csv',\n",
       " 'rawdata/original_raw_files/full_data/Medical Claims.csv',\n",
       " 'rawdata/original_raw_files/full_data/Not sure.csv',\n",
       " 'rawdata/original_raw_files/full_data/Others.csv',\n",
       " 'rawdata/original_raw_files/full_data/Pharm.csv',\n",
       " 'rawdata/original_raw_files/full_data/dependent.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_full_data = 'rawdata/original_raw_files/full_data'\n",
    "prefix_train = 'rawdata/original_raw_files/train'\n",
    "prefix_valid = 'rawdata/original_raw_files/valid'\n",
    "bucket = \"humana-data\"\n",
    "\n",
    "\n",
    "conn = boto3.client('s3')\n",
    "contents = conn.list_objects(Bucket=bucket, Prefix=prefix_full_data)['Contents']\n",
    "\n",
    "rawfile_names = [key['Key'] for key in contents]\n",
    "rawfile_names = [x for x in rawfile_names if x.endswith('.csv')]\n",
    "rawfile_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3276242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful S3 get_object response. Status - 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'MXR3MJ46VJRYS4KB',\n",
       "  'HostId': '9I0kJQz/42i1tfrui7xzh+Y1GFR7sbLQHR/14BlLRmFIoFVIPWdYrbiSp7T6rnqid0wA1r/6O6E=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '9I0kJQz/42i1tfrui7xzh+Y1GFR7sbLQHR/14BlLRmFIoFVIPWdYrbiSp7T6rnqid0wA1r/6O6E=',\n",
       "   'x-amz-request-id': 'MXR3MJ46VJRYS4KB',\n",
       "   'date': 'Tue, 29 Nov 2022 20:07:15 GMT',\n",
       "   'x-amz-version-id': 'nx2qigYqTl9DGuQbngoRdXD65xlPtvsh',\n",
       "   'etag': '\"9bb908fd62c9aa4ecd7b726b083f87ce\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"9bb908fd62c9aa4ecd7b726b083f87ce\"',\n",
       " 'VersionId': 'nx2qigYqTl9DGuQbngoRdXD65xlPtvsh'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the demo file\n",
    "response = conn.get_object(Bucket=bucket, Key='rawdata/original_raw_files/full_data/Condition.csv')\n",
    "status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "if status == 200:\n",
    "    print(f\"Successful S3 get_object response. Status - {status}\")\n",
    "    condition_df = pd.read_csv(response.get(\"Body\"))\n",
    "else:\n",
    "    print(f\"Unsuccessful S3 get_object response. Status - {status}\")\n",
    "\n",
    "condition_df.columns = condition_df.columns.str.lower()\n",
    "condition_df = condition_df.set_index('person_id_syn')\n",
    "\n",
    "# condition_df = dependent_df.merge(condition_df, left_index=True, right_index=True)\n",
    "\n",
    "condition_train_df = condition_df.sample(frac= 0.7)\n",
    "condition_train_df = condition_train_df.sample(10000)\n",
    "train_indexes = pd.DataFrame(condition_train_df.reset_index()['person_id_syn'].to_dict().items(),\n",
    "                             columns = ['index_num', 'person_id_syn'])\n",
    "\n",
    "\n",
    "condition_valid_df = condition_df[~(condition_df.index.isin(train_indexes['person_id_syn']))]\n",
    "condition_valid_df = condition_valid_df.sample(4000)\n",
    "valid_indexes = pd.DataFrame(condition_valid_df.reset_index()['person_id_syn'].to_dict().items(), columns = ['index_num', 'person_id_syn'])\n",
    "\n",
    "# Saving train\n",
    "# filename = 'condition.parquet.gzip'\n",
    "# s3_train_url = 's3://{}/{}/{}'.format(bucket, prefix_train, filename)\n",
    "# s3_valid_url = 's3://{}/{}/{}'.format(bucket, prefix_valid, filename)\n",
    "# condition_train_df.to_parquet(s3_train_url, compression='gzip')\n",
    "# condition_valid_df.to_parquet(s3_valid_url, compression='gzip')\n",
    "\n",
    "\n",
    "from io import StringIO # python3; python2: BytesIO \n",
    "csv_buffer = StringIO()\n",
    "condition_train_df.to_csv(csv_buffer, index=False)\n",
    "s3_resource = boto3.resource('s3')\n",
    "filename_condition = \"condition.csv\"\n",
    "\n",
    "fe_path = prefix_train + '/' + filename_condition\n",
    "s3_resource.Object(bucket, fe_path).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "condition_valid_df.to_csv(csv_buffer, index=False)\n",
    "s3_resource = boto3.resource('s3')\n",
    "filename_condition = \"condition.csv\"\n",
    "\n",
    "fe_path = prefix_valid + '/' + filename_condition\n",
    "s3_resource.Object(bucket, fe_path).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76fcab7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful S3 get_object response. Status - 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'QHPVQ612VYCQ50TN',\n",
       "  'HostId': '8h9E/xVFpYJNWOEf+rgAggj7opln/YbWR9BfSx0x8FmHGQ31BfzZ0VbYTHYQedFywmD/2FsX1/s=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '8h9E/xVFpYJNWOEf+rgAggj7opln/YbWR9BfSx0x8FmHGQ31BfzZ0VbYTHYQedFywmD/2FsX1/s=',\n",
       "   'x-amz-request-id': 'QHPVQ612VYCQ50TN',\n",
       "   'date': 'Tue, 29 Nov 2022 20:10:33 GMT',\n",
       "   'x-amz-version-id': '7wp2yNPIe9b8SyPNxkeaqlvhBOBzRKAd',\n",
       "   'etag': '\"609483fd6d4c52b1fc1eb4fe9e53ac59\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"609483fd6d4c52b1fc1eb4fe9e53ac59\"',\n",
       " 'VersionId': '7wp2yNPIe9b8SyPNxkeaqlvhBOBzRKAd'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dependent\n",
    "# Get the demo file\n",
    "response = conn.get_object(Bucket=bucket, Key='rawdata/original_raw_files/full_data/dependent.csv')\n",
    "status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "if status == 200:\n",
    "    print(f\"Successful S3 get_object response. Status - {status}\")\n",
    "    dependent_df = pd.read_csv(response.get(\"Body\"))\n",
    "else:\n",
    "    print(f\"Unsuccessful S3 get_object response. Status - {status}\")\n",
    "    \n",
    "    \n",
    "dependent_df.columns = dependent_df.columns.str.lower()\n",
    "dependent_df = dependent_df.set_index('person_id_syn')\n",
    "\n",
    "dependent_train_df = dependent_df[(dependent_df.index.isin(train_indexes['person_id_syn']))]\n",
    "dependent_valid_df = dependent_df[(dependent_df.index.isin(valid_indexes['person_id_syn']))]\n",
    "\n",
    "\n",
    "# filename = 'dependent.parquet.gzip'\n",
    "# s3_train_url = 's3://{}/{}/{}'.format(bucket, prefix_train, filename)\n",
    "# s3_valid_url = 's3://{}/{}/{}'.format(bucket, prefix_valid, filename)\n",
    "# dependent_train_df.to_parquet(s3_train_url, compression='gzip')\n",
    "# dependent_valid_df.to_parquet(s3_valid_url, compression='gzip')\n",
    "\n",
    "from io import StringIO # python3; python2: BytesIO \n",
    "csv_buffer = StringIO()\n",
    "dependent_train_df.to_csv(csv_buffer, index=False)\n",
    "s3_resource = boto3.resource('s3')\n",
    "filename_dependent = \"dependent.csv\"\n",
    "\n",
    "fe_path = prefix_train + '/' + filename_dependent\n",
    "s3_resource.Object(bucket, fe_path).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "dependent_valid_df.to_csv(csv_buffer, index=False)\n",
    "s3_resource = boto3.resource('s3')\n",
    "filename_dependent = \"dependent.csv\"\n",
    "\n",
    "fe_path = prefix_valid + '/' + filename_dependent\n",
    "s3_resource.Object(bucket, fe_path).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a50873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6beb3857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving index as parquet files\n",
    "filename = 'index_person_id_syn.parquet.gzip'\n",
    "s3_train_url = 's3://{}/{}/{}'.format(bucket, prefix_train, filename)\n",
    "s3_valid_url = 's3://{}/{}/{}'.format(bucket, prefix_valid, filename)\n",
    "\n",
    "\n",
    "train_indexes.to_parquet(s3_train_url, compression='gzip')\n",
    "valid_indexes.to_parquet(s3_valid_url, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6358d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition_valid_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73a5a845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful S3 get_object response. Status - 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '79H7KARMKZFSRPJJ',\n",
       "  'HostId': 'OcKSIJx7XBBR2sMxjwfqHPWNH6764INmq0QKCYVjwvBTCMIqByZ5Fd3VG22/KxN50jjH9e3FQ1c=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'OcKSIJx7XBBR2sMxjwfqHPWNH6764INmq0QKCYVjwvBTCMIqByZ5Fd3VG22/KxN50jjH9e3FQ1c=',\n",
       "   'x-amz-request-id': '79H7KARMKZFSRPJJ',\n",
       "   'date': 'Tue, 29 Nov 2022 20:12:32 GMT',\n",
       "   'x-amz-version-id': 'MFda9En29TskLn6BzfjYcUSFR3CgZnHR',\n",
       "   'etag': '\"7c67a0113e29e76e2b113ed3407b90a7\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"7c67a0113e29e76e2b113ed3407b90a7\"',\n",
       " 'VersionId': 'MFda9En29TskLn6BzfjYcUSFR3CgZnHR'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Credit\n",
    "\n",
    "# Get the demo file\n",
    "response = conn.get_object(Bucket=bucket, Key='rawdata/original_raw_files/full_data/Credit.csv')\n",
    "status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "if status == 200:\n",
    "    print(f\"Successful S3 get_object response. Status - {status}\")\n",
    "    credit_df = pd.read_csv(response.get(\"Body\"))\n",
    "else:\n",
    "    print(f\"Unsuccessful S3 get_object response. Status - {status}\")\n",
    "    \n",
    "    \n",
    "credit_df.columns = credit_df.columns.str.lower()\n",
    "credit_df = credit_df.set_index('person_id_syn')\n",
    "# credit_df = credit_df.merge(condition_df, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "credit_train_df = credit_df[(credit_df.index.isin(train_indexes['person_id_syn']))]\n",
    "credit_valid_df = credit_df[(credit_df.index.isin(valid_indexes['person_id_syn']))]\n",
    "\n",
    "\n",
    "# filename = 'credit.parquet.gzip'\n",
    "# s3_train_url = 's3://{}/{}/{}'.format(bucket, prefix_train, filename)\n",
    "# s3_valid_url = 's3://{}/{}/{}'.format(bucket, prefix_valid, filename)\n",
    "# credit_train_df.to_parquet(s3_train_url, compression='gzip')\n",
    "# credit_valid_df.to_parquet(s3_valid_url, compression='gzip')\n",
    "from io import StringIO # python3; python2: BytesIO \n",
    "csv_buffer = StringIO()\n",
    "credit_train_df.to_csv(csv_buffer, index=False)\n",
    "s3_resource = boto3.resource('s3')\n",
    "filename_credit = \"credit.csv\"\n",
    "fe_path = prefix_train + '/' + filename_credit\n",
    "s3_resource.Object(bucket, fe_path).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "credit_valid_df.to_csv(csv_buffer, index=False)\n",
    "s3_resource = boto3.resource('s3')\n",
    "filename_credit = \"credit.csv\"\n",
    "fe_path = prefix_valid + '/' + filename_credit\n",
    "s3_resource.Object(bucket, fe_path).put(Body=csv_buffer.getvalue())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f3913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d08b11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee586f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f5133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
